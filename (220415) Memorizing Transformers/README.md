# Memorizing Transformers (리뷰)
Long-Context Language Modeling을 위해 Attention 연산으로 참조 가능한 External Memory (Key-Values)를 활용하는 Decoder-Only Transformer 모델. Learnable하지 않은 External Memory 특성상 효율적인 학습이 가능하고, 성능 향상이 Consistent한 점이 인상적. 일반적인 Long-Context 처리를 위한 모델들 (Longformer, Big Bird 등)과 색다른 느낌으로 흥미로웠음.<br/><br/>
참조한 논문들:
* Memorizing Transformers ([arXiv](https://arxiv.org/abs/2203.08913))
* Augmenting Self-attention with Persistent Memory ([arXiv](https://arxiv.org/abs/1907.01470))
* Large Memory Layers with Product Keys ([arXiv](https://arxiv.org/abs/1907.05242))
* Generalization through Memorization: Nearest Neighbor Language Models ([arXiv](https://arxiv.org/abs/1911.00172))

상세한 [Review](https://chainsmokers.oopy.io/paper/memorizing-transformers)

