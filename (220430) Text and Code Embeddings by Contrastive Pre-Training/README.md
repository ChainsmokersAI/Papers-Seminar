# Text and Code Embeddings by Contrastive Pre-Training (CPT, 리뷰)
SimCSE와 함께 리뷰함. Transfer Learning을 전제로 하는 Transformer-based LM (BERT, RoBERTa, GPT 등)을 순수 Sentence Embedding 모델로 활용하기 위한 Contrastive Pre-Training 방법론. SBERT와는 달리, Negative Samples를 활용하여 Sentence Embedding의 Uniformity를 확보하는 데에 초점을 맞춤. 간단하면서도 좋은 성능으로 실제로 활용하기 좋아 보임.<br/><br/>
References:
* [CPT](https://arxiv.org/abs/2201.10005)
* [SimCSE](https://arxiv.org/abs/2104.08821)

상세한 [Review](https://chainsmokers.oopy.io/paper/simcse-cpt)

